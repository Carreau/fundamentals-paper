\section{Quality Assurance}
% Organize this section according to major topics
% give each topic a section heading in boldface.
% try to cover the major common points :
%
% problem design
% methods of measurement
% supporting models
% supporting data
% simulations run
% results

% Just write the section headings for each part and indicate what goes in that
% section with words :
%
% heading
% figures (with captions)
% schematics (with captions and footnotes)
% equations
% tables

% What does it mean?
% What did I actually test?
% What were the results?
% Did the work yield a new method?
% Did the work yield new knowledge?
% What measurements did I make?
% How were these measurements characterized?
% What methods were used?
% What were the results?
% How were the measurements made and characterized?

Simulation science - like experimental science - has since the begining had the 
problem of identifying trustworthy results from the nonsensical and the noise.
This is epitomized in the Charles Babbage quote, ``On two occasions I have been asked, 
`Pray, Mr. Babbage, if you put into the machine wrong figures, will the right 
answers come out?' ... I am not able rightly to apprehend the kind of confusion 
of ideas that could provoke such a question.'' \cite{babbage2011passages}. 
The \emph{garbage in, garbage out} phenomenon is not the only scenario which a
simulator must gaurd against; ensuring correctness is equally important.

Multiple strategies collectively known a \emph{quality assurance} (QA) have 
been invented over the years to mitigate the structural and algorithmic errors
on the part of simulators themselves. These include \emph{verification and validation}
(V\&V) \cite{boehm1989software}, \emph{uncertainty quantification} (UQ) 
\cite{sacks1989design}, testing, and others. V\&V and UQ come from 
a distinctly computational science background while testing \emph{et al.} come from 
a software development bent. 

Nuclear enginerering code quality is often governed by NQA-1, an ASME specification 
whose latest revision appeared in 2009 \cite{NQA-1a-2009}. This is primarily 
used for designing reactors. However, it is general enough to apply to 
cyclus. Cyclus has adopted an \emph{agile} development process \cite{larman2004agile}, 
interpreting NQA-1 in a manner similar to NEAMS \cite{neams-qa} or PyNE \cite{pyne-vv}. 

Cyclus acknowledges that quality assurance is an on-going process throughout the 
entire life of the code. As a simulator where most modeling descisions are made 
by third party archetype developers or users, the most important feature of QA 
is verification. Validation may be impossible and UQ is beyond the scope.

Verification may be defined as the question, ``Is cyclus being built correctly?'' 
To answer this question we turn to the software development of notions of testing,
documentation, version control, style guidelines, and continuous integration. 
This suite of process controls supplies mechanims for reliable and reproducible 
software. The impetus to implement these correctly is even stronger in a scientific 
context because of the emphasis on reproducibility and provenance. These are 
discussed below individually.

Validation on the other hand may be defined as the question, 
``Is cyclus the correct tool?''
Since cyclus is alone in its class as an agent-based fuel cycle simulator logitudnal 
validation is not possible. Still code-to-code comparisons with fuel cycle
simulators with other modeling paradigms are underway, if nascent. However, such 
exercises are more likely to bring into relief the differences between the modeling
paradigms than be useful for QA and validation. 

Lastly, uncertainty quantification is a process that is used on specific simulation
instantiations to statistically determine its quality. Since the cyclus 
kernel requires an input file written by the user, UQ is a process that applies 
more to those running cyclus than to those running it.  Furthermore, any UQ
results that are generated are applicable only to the scenario that is under 
analysis. Thus for cyclus UQ is well beyond the scope of core development.

\subsection{Testing}

Automated software \emph{testing} is the first line of defense against
errors in implementation. It also acts as an early warning sign that the
simulator does or does not work as intended on a new system.
Testing serves a critical role in QA because it directly compares the 
results of running software versus the expected behavior of the software.
In cyclus, three categories of tests are defined: unit tests, integration 
tests, and regression tests. 

It is important to note that before a proposed
code changes is allowed into main stream cyclus, all of the tests must pass
and the change must be covered by a test, either new or existing. This is 
matter of policy for the developers. Additionally, test code is equally likely 
as the code it is testing to contain errors. To prevent an infinite recurrsion 
of testing-the-tests, it is assumed that dilligence and one level of testing 
is sufficient to meet the aims of the quality assurance.

\subsubsection{Unit Tests}

Unit tests are those which compare the results of the smallest code \emph{unit}, 
typically a single function or a class. What constitutes a the smallest code
unit depends on the context of the specific unit in question. While this is 
an ambiquous defintiion, combined with a philosophy of keeping units as small
as possible it is often clear in practice what needs to be tested.

Cyclus uses the Google Test framework \cite{gtest} as a harness for running unit 
tests. Sufficient unit tests are required for any proposed change to the cyclus
code base. Currently cyclus implements over 450 unit tests and cycamore has 
85.  These cover approximately 65\% of their respective code bases. This number 
is expected to grow over time. 

\subsubsection{Integration Tests} 

Intergration tests are any tests that combine multiple elemnents of the 
cyclus interface and test that they work correctly with each other.  By analogy, 
simply because the gears (units) are made correctly does not imply that the 
clock (integration) will run smoothly, run at all, or give the correct time.
In cyclus and cycamore, integration tests are performed by running sample
simulations and verifying that results are what would be expected ahead of 
time. Precanned input files have thus been contsructed. Their results 
are inspected and compared via nose \cite{nosetests}, a Python test framework.
Python will run cyclus as a subprocess for each integration test. In this
way cyclus code units are tested in the full context that they will
executed. This category of testing is especially useful for ensuring that 
major cyclus components are functioning as expected.

\subsubsection{Regression Tests}

Regression tests are tests which ensure that significant changes do not 
occur over the course of cyclus development. Such a change is called a 
\emph{regression} because the new version is almost always wrong.
Regression tests are implemeneted similarly to integration tests.
Nose is used to execute fully fledged cyclus simulations whose results
are then compared. In this category however the comparison is done against 
the output of the same input file when run with a previous version of cyclus, 
typically the last released version.
In some sense, regression tests are `dumb' in that they do 
not care about the contents of a simulation being correct, only whether or not 
it changed. They thus pick up glaring time-sensitive mistakes that may 
not appear elsewhere though they do not purport to make claims about the
quality of the physics modeled. The cycamore project is the primary location of
regression tests. This is because the archetypes here are sophisticated enough
to make interesting input files. Only a handful ($<10$) time steps are needed 
for regression testing.


